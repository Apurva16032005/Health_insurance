{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aad83be4-c73c-4cb8-9506-b3da5f73e284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NLP Imports Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from utils.preprocess import clean_extracted_text\n",
    "from utils.extract_fields import extract_all_fields\n",
    "from models.nlp_model import analyze_claim_description\n",
    "\n",
    "print(\"✓ NLP Imports Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5eb7cc1-67d3-4bce-8ee2-88bdef6b3c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: D:\\Desktop\\insurance-claim-checker\n",
      "DATA: D:\\Desktop\\insurance-claim-checker\\data\n",
      "OCR_PATH: D:\\Desktop\\insurance-claim-checker\\data\\ocr_output.pkl\n",
      "NLP_PATH: D:\\Desktop\\insurance-claim-checker\\data\\nlp_output.pkl\n"
     ]
    }
   ],
   "source": [
    "# Base folder = insurance-claim-checker\n",
    "BASE = Path().resolve().parents[1]\n",
    "\n",
    "DATA_DIR = BASE / \"data\"\n",
    "OCR_PATH = DATA_DIR / \"ocr_output.pkl\"\n",
    "NLP_PATH = DATA_DIR / \"nlp_output.pkl\"\n",
    "\n",
    "print(\"BASE:\", BASE)\n",
    "print(\"DATA:\", DATA_DIR)\n",
    "print(\"OCR_PATH:\", OCR_PATH)\n",
    "print(\"NLP_PATH:\", NLP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d51f5711-a243-469f-81e3-13ef41f327dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OCR entries: 2\n"
     ]
    }
   ],
   "source": [
    "if not OCR_PATH.exists():\n",
    "    raise FileNotFoundError(f\"❌ OCR file not found at: {OCR_PATH}\\nRun OCR notebook first!\")\n",
    "\n",
    "with open(OCR_PATH, \"rb\") as f:\n",
    "    ocr_data = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded OCR entries: {len(ocr_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b5b867a-16c6-4742-9418-097df2a1f66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DistilBERT loaded\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"✓ DistilBERT loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6890defb-df98-49c9-b13b-957f0ac65f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    text = clean_extracted_text(text)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    vec = output.squeeze().numpy()\n",
    "\n",
    "    # Guarantee shape = 768\n",
    "    if vec.shape[0] != 768:\n",
    "        padded = np.zeros(768)\n",
    "        padded[:min(768, len(vec))] = vec[:768]\n",
    "        vec = padded\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5dc096f0-db92-4af1-a2bd-babf17ca30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ NLP Completed for 2 files\n"
     ]
    }
   ],
   "source": [
    "nlp_output = {}\n",
    "processed = 0\n",
    "\n",
    "for filename, ocr_entry in ocr_data.items():\n",
    "\n",
    "    # Validate structure\n",
    "    if not isinstance(ocr_entry, dict):\n",
    "        print(f\"Skipping: {filename} (not a dict)\")\n",
    "        continue\n",
    "\n",
    "    if \"clean_text\" not in ocr_entry:\n",
    "        print(f\"Skipping: {filename} (missing clean_text)\")\n",
    "        continue\n",
    "\n",
    "    raw_text = ocr_entry[\"clean_text\"]\n",
    "\n",
    "    if not isinstance(raw_text, str):\n",
    "        print(f\"Skipping: {filename} (clean_text not string)\")\n",
    "        continue\n",
    "\n",
    "    cleaned = clean_extracted_text(raw_text)\n",
    "    embedding = get_embedding(cleaned)\n",
    "    fields = extract_all_fields(cleaned)\n",
    "    analysis = analyze_claim_description(cleaned)\n",
    "\n",
    "    nlp_output[filename] = {\n",
    "        \"clean_text\": cleaned,\n",
    "        \"embedding\": embedding,\n",
    "        \"fields\": fields,\n",
    "        \"analysis\": analysis,\n",
    "        \"severity_score\": analysis[\"severity_score\"]\n",
    "    }\n",
    "\n",
    "    processed += 1\n",
    "\n",
    "print(f\"\\n✓ NLP Completed for {processed} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "50c8fdd3-eaa6-42f7-8553-4ce824b02bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP saved to: D:\\Desktop\\insurance-claim-checker\\data\\nlp_output.pkl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "with open(NLP_PATH, \"wb\") as f:\n",
    "    pickle.dump(nlp_output, f)\n",
    "\n",
    "print(\"NLP saved to:\", NLP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c6e99-0e7b-4f5d-8b21-e00bedf417b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Claim Checker venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
